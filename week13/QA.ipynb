{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5602d9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import collections\n",
    "import itertools\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4a3cf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(infile):\n",
    "  stories, questions, answers =[], [], []\n",
    "  story_text = []\n",
    "  fin = open(Train_File, 'rb')\n",
    "  for line in fin:\n",
    "    line = line.decode(\"utf-8\").strip()\n",
    "    lno, text = line.split(\" \",1)\n",
    "    if \"\\t\" in text:\n",
    "      question, answer, _ = text.split(\"\\t\")\n",
    "      stories.append(story_text)\n",
    "      questions.append(question)\n",
    "      answers.append(answer)\n",
    "      story_text = []\n",
    "    else:\n",
    "      story_text.append(text)\n",
    "  fin.close()\n",
    "  return stories, questions, answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed8dbbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train observations: 10000 Test observations: 10000\n"
     ]
    }
   ],
   "source": [
    "Train_File = os.path.join(\"qa1_single-supporting-fact_train.txt\")\n",
    "Test_File = os.path.join(\"qa1_single-supporting-fact_test.txt\")\n",
    "\n",
    "data_train = get_data(Train_File)\n",
    "data_test = get_data(Test_File)\n",
    "\n",
    "print('Train observations:', len(data_train[0]), 'Test observations:',len(data_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e17701d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 22\n"
     ]
    }
   ],
   "source": [
    "dictnry = collections.Counter()\n",
    "for stories, questions, answers in [data_train, data_test]:\n",
    "  for story in stories:\n",
    "    for sent in story:\n",
    "      for word in nltk.word_tokenize(sent):\n",
    "        dictnry[word.lower()] += 1\n",
    "  for question in questions:\n",
    "    for word in nltk.word_tokenize(question):\n",
    "      dictnry[word.lower()] += 1\n",
    "  for answer in  answers:\n",
    "    for word in nltk.word_tokenize(answer):\n",
    "      dictnry[word.lower()] += 1\n",
    "\n",
    "word2indx = {w:(i+1) for i,(w,_) in enumerate(dictnry.most_common())}\n",
    "\n",
    "word2indx[\"PAD\"] = 0\n",
    "indx2word = {v:k for k,v in word2indx.items()}\n",
    "\n",
    "vocab_size = len(word2indx)\n",
    "print('vocabulary size:', len(word2indx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84b4be93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story maximm length: 0 Question maximum length: 4\n"
     ]
    }
   ],
   "source": [
    "story_maxlen = 0\n",
    "question_maxlen = 0\n",
    "\n",
    "for stories, questions, answers in [data_train, data_test]:\n",
    "  for story in stories:\n",
    "    story_len = 0\n",
    "    for sent in story:\n",
    "      swords = nltk.word_tokenize(sent)\n",
    "      story_len += len(swords)\n",
    "    if story_len > story_maxlen:\n",
    "      story_maxlen = story_maxlen\n",
    "\n",
    "  for question in questions:\n",
    "    question_len = len(nltk.word_tokenize(question))\n",
    "    if question_len > question_maxlen:\n",
    "      question_maxlen = question_len\n",
    "\n",
    "print('Story maximm length:', story_maxlen, 'Question maximum length:', question_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2822291",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "from keras.layers.core import Activation, Dense, Dropout, Permute\n",
    "from keras.layers.embeddings import  Embedding\n",
    "from keras.layers.merge import add, concatenate, dot\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c27d0dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_vectorization(data, word2idx, story_maxlen, question_maxlen):\n",
    "    Xs, Xq, Y = [], [], []\n",
    "    stories, questions, answers = data\n",
    "    for story, question, answer in zip(stories, questions, answers):\n",
    "        xs = [[word2indx[w.lower()] for w in nltk.word_tokenize(s)] for s in story]\n",
    "        xs = list(itertools.chain.from_iterable(xs))\n",
    "        xq = [word2indx[w.lower()] for w in nltk.word_tokenize(question)]\n",
    "        Xs.append(xs)\n",
    "        Xq.append(xq)\n",
    "        Y.append(word2indx[answer.lower()])\n",
    "\n",
    "    return pad_sequences(Xs, maxlen=story_maxlen), pad_sequences(Xq, maxlen=qusestion_maxlen), np_utils.to_categorical(Y, num_classes=len(word2indx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0258bf61",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (12,) into shape (0,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-73d9cecaf5c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mXstrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXqtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_vectorization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2indx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstory_maxlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestion_maxlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mXstest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXqtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_vectorization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2indx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstory_maxlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestion_maxlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Train story\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXstrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Train question\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXqtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Train answer\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Test story\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXstest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Test question\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXqtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Test answer\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-360e3b63b209>\u001b[0m in \u001b[0;36mdata_vectorization\u001b[1;34m(data, word2idx, story_maxlen, question_maxlen)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword2indx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstory_maxlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mqusestion_maxlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword2indx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m    150\u001b[0m           \u001b[1;32mor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcase\u001b[0m \u001b[0mof\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mentry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m   \"\"\"\n\u001b[1;32m--> 152\u001b[1;33m   return sequence.pad_sequences(\n\u001b[0m\u001b[0;32m    153\u001b[0m       \u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m       padding=padding, truncating=truncating, value=value)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras_preprocessing\\sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mpadding\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'pre'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m             \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Padding type \"%s\" not understood'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (12,) into shape (0,)"
     ]
    }
   ],
   "source": [
    "Xstrain, Xqtrain, Ytrain = data_vectorization(data_train, word2indx, story_maxlen, question_maxlen)\n",
    "Xstest, Xqtest, Ytest = data_vectorization(data_test, word2indx, story_maxlen, question_maxlen)\n",
    "\n",
    "print(\"Train story\", Xstrain.shape, \"Train question\", Xqtrain.shape, \"Train answer\", Ytrain.shape)\n",
    "print(\"Test story\", Xstest.shape, \"Test question\", Xqtest.shape, \"Test answer\", Ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013ef3c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
