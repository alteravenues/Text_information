{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25ea22f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fd23560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\alter\\anaconda3\\lib\\site-packages (2.7.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e24bec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.7.0-cp38-cp38-win_amd64.whl (430.8 MB)\n",
      "Collecting absl-py>=0.4.0\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\alter\\anaconda3\\lib\\site-packages (from tensorflow) (1.20.1)\n",
      "Collecting libclang>=9.0.1\n",
      "  Downloading libclang-12.0.0-py2.py3-none-win_amd64.whl (13.1 MB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.19.1-cp38-cp38-win_amd64.whl (895 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.21.0\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.22.0-cp38-cp38-win_amd64.whl (1.5 MB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\alter\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.41.1-cp38-cp38-win_amd64.whl (3.2 MB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
      "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\alter\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting tensorboard~=2.6\n",
      "  Downloading tensorboard-2.7.0-py3-none-any.whl (5.8 MB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in c:\\users\\alter\\anaconda3\\lib\\site-packages (from tensorflow) (0.36.2)\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in c:\\users\\alter\\anaconda3\\lib\\site-packages (from tensorflow) (2.7.0)\n",
      "Collecting gast<0.5.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\alter\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Collecting flatbuffers<3.0,>=1.12\n",
      "  Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\alter\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\alter\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.25.1)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.3.3-py2.py3-none-any.whl (155 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\alter\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (52.0.0.post20210125)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\alter\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\alter\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\alter\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alter\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\alter\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.4)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=32241a136ad3d83b68b0275b30b4e011ec99b11e8286f0c11e58098195aacaaa\n",
      "  Stored in directory: c:\\users\\alter\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "Successfully built termcolor\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, tensorboard-plugin-wit, tensorboard-data-server, protobuf, markdown, grpcio, google-auth-oauthlib, absl-py, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras-preprocessing, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-4.2.4 flatbuffers-2.0 gast-0.4.0 google-auth-2.3.3 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.41.1 keras-preprocessing-1.1.2 libclang-12.0.0 markdown-3.3.4 oauthlib-3.1.1 opt-einsum-3.3.0 protobuf-3.19.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.7.2 tensorboard-2.7.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.7.0 tensorflow-estimator-2.7.0 tensorflow-io-gcs-filesystem-0.22.0 termcolor-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab1b4205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20개 카테고리 전체 목록:\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "\n",
      "\n",
      "샘플 이메일\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "샘플 타깃 카테고리:\n",
      "7\n",
      "rec.autos\n"
     ]
    }
   ],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset = 'test')\n",
    "\n",
    "x_train = newsgroups_train.data\n",
    "x_test = newsgroups_test.data\n",
    "\n",
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target\n",
    "\n",
    "print(\"20개 카테고리 전체 목록:\")\n",
    "print(newsgroups_train.target_names)\n",
    "print(\"\\n\")\n",
    "print(\"샘플 이메일\")\n",
    "print(x_train[0])\n",
    "print(\"샘플 타깃 카테고리:\")\n",
    "print(y_train[0])\n",
    "print(newsgroups_train.target_names[y_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9cfd5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3b9b898",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alter\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alter\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\alter\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alter\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c93e624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "  text2 = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in text]).split())\n",
    "  tokens = [word for sent in nltk.sent_tokenize(text2) for word in nltk.word_tokenize(sent)]\n",
    "\n",
    "  stopwds = stopwords.words('english')\n",
    "  tokens = [token for token in tokens if token not in stopwds]\n",
    "  tokens = [word for word in tokens if len(word)>=3]\n",
    "\n",
    "  stemmer = PorterStemmer()\n",
    "  tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "  tagged_corpus = pos_tag(tokens)\n",
    "\n",
    "  Noun_tags = ['NN', 'NNP', 'NNPS', 'NNS']\n",
    "  Verb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "  def prat_lemmatize(token, tag):\n",
    "    if tag in Noun_tags:\n",
    "      return lemmatizer.lemmatize(token,'n')\n",
    "    elif tag in Verb_tags:\n",
    "      return lemmatizer.lemmatize(token, 'v')\n",
    "    else:\n",
    "      return lemmatizer.lemmatize(token, 'n')\n",
    "\n",
    "  pre_proc_text = \" \".join([prat_lemmatize(token, tag) for token,tag in tagged_corpus])\n",
    "\n",
    "  return pre_proc_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88fab052",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_preprocessed = []\n",
    "for i in x_train:\n",
    "  x_train_preprocessed.append(preprocessing(i))\n",
    "\n",
    "x_test_preprocessed = []\n",
    "for i in x_test:\n",
    "  x_test_preprocessed.append(preprocessing(i))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=2, ngram_range=(1,2), stop_words='english', max_features=10000, strip_accents='unicode', norm='l2')\n",
    "\n",
    "x_train_2 = vectorizer.fit_transform(x_train_preprocessed).todense()\n",
    "x_test_2 = vectorizer.transform(x_test_preprocessed).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e56c2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 1000)              10001000  \n",
      "                                                                 \n",
      " activation (Activation)     (None, 1000)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 500)               500500    \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 500)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 500)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 50)                25050     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 50)                0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 20)                1020      \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 20)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,527,570\n",
      "Trainable params: 10,527,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from tensorflow.keras.optimizers import Adadelta, Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "\n",
    "np.random.seed(1337)\n",
    "nb_classes = 20\n",
    "batch_size = 64\n",
    "nb_epochs = 20\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(1000, input_shape=(10000,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(50))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9c7489d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "177/177 [==============================] - 6s 29ms/step - loss: 1.9531\n",
      "Epoch 2/20\n",
      "177/177 [==============================] - 5s 29ms/step - loss: 0.6071\n",
      "Epoch 3/20\n",
      "177/177 [==============================] - 5s 30ms/step - loss: 0.3079\n",
      "Epoch 4/20\n",
      "177/177 [==============================] - 5s 29ms/step - loss: 0.1781\n",
      "Epoch 5/20\n",
      "177/177 [==============================] - 5s 29ms/step - loss: 0.1168\n",
      "Epoch 6/20\n",
      "177/177 [==============================] - 5s 31ms/step - loss: 0.0844\n",
      "Epoch 7/20\n",
      "177/177 [==============================] - 5s 30ms/step - loss: 0.0670\n",
      "Epoch 8/20\n",
      "177/177 [==============================] - 5s 29ms/step - loss: 0.0604\n",
      "Epoch 9/20\n",
      "177/177 [==============================] - 5s 30ms/step - loss: 0.0503\n",
      "Epoch 10/20\n",
      "177/177 [==============================] - 5s 29ms/step - loss: 0.0452\n",
      "Epoch 11/20\n",
      "177/177 [==============================] - 5s 29ms/step - loss: 0.0381\n",
      "Epoch 12/20\n",
      "177/177 [==============================] - 5s 29ms/step - loss: 0.0357\n",
      "Epoch 13/20\n",
      "177/177 [==============================] - 5s 29ms/step - loss: 0.0333\n",
      "Epoch 14/20\n",
      "177/177 [==============================] - 5s 29ms/step - loss: 0.0311\n",
      "Epoch 15/20\n",
      "177/177 [==============================] - 5s 29ms/step - loss: 0.0303\n",
      "Epoch 16/20\n",
      "177/177 [==============================] - 5s 29ms/step - loss: 0.0275\n",
      "Epoch 17/20\n",
      "177/177 [==============================] - 5s 29ms/step - loss: 0.0300\n",
      "Epoch 18/20\n",
      "177/177 [==============================] - 5s 29ms/step - loss: 0.0270\n",
      "Epoch 19/20\n",
      "177/177 [==============================] - 5s 29ms/step - loss: 0.0211\n",
      "Epoch 20/20\n",
      "177/177 [==============================] - 5s 29ms/step - loss: 0.0216\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2796ddd2cd0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_2, Y_train, batch_size=batch_size, epochs=nb_epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f47706dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train_predclass = model.predict_classes(x_train_2,batch_size=batch_size)\n",
    "#y_test_predclass = model.predict_classes(x_test_2, batch_size=batch_size)\n",
    "y_train_predclass = np.argmax(model.predict(x_train_2),axis=1)\n",
    "y_test_predclass = np.argmax(model.predict(x_test_2),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "846d4475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Deep Neural Network - Train accuracy: 0.999\n",
      "\n",
      "Deep Neural Network - Test accuracy: 0.805\n",
      "\n",
      "Deep Neural Network - Train Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       480\n",
      "           1       1.00      1.00      1.00       584\n",
      "           2       1.00      1.00      1.00       591\n",
      "           3       1.00      1.00      1.00       590\n",
      "           4       1.00      1.00      1.00       578\n",
      "           5       1.00      1.00      1.00       593\n",
      "           6       1.00      1.00      1.00       585\n",
      "           7       1.00      1.00      1.00       594\n",
      "           8       1.00      1.00      1.00       598\n",
      "           9       1.00      1.00      1.00       597\n",
      "          10       1.00      1.00      1.00       600\n",
      "          11       1.00      1.00      1.00       595\n",
      "          12       0.99      1.00      1.00       591\n",
      "          13       1.00      1.00      1.00       594\n",
      "          14       1.00      1.00      1.00       593\n",
      "          15       1.00      1.00      1.00       599\n",
      "          16       1.00      1.00      1.00       546\n",
      "          17       1.00      1.00      1.00       564\n",
      "          18       1.00      1.00      1.00       465\n",
      "          19       1.00      1.00      1.00       377\n",
      "\n",
      "    accuracy                           1.00     11314\n",
      "   macro avg       1.00      1.00      1.00     11314\n",
      "weighted avg       1.00      1.00      1.00     11314\n",
      "\n",
      "\n",
      "Deep Neural Network - Test classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.72      0.77       319\n",
      "           1       0.71      0.70      0.71       389\n",
      "           2       0.66      0.71      0.68       394\n",
      "           3       0.65      0.68      0.67       392\n",
      "           4       0.74      0.77      0.76       385\n",
      "           5       0.81      0.75      0.78       395\n",
      "           6       0.85      0.79      0.82       390\n",
      "           7       0.87      0.83      0.85       396\n",
      "           8       0.88      0.93      0.90       398\n",
      "           9       0.89      0.89      0.89       397\n",
      "          10       0.91      0.96      0.94       399\n",
      "          11       0.95      0.90      0.93       396\n",
      "          12       0.67      0.73      0.70       393\n",
      "          13       0.89      0.84      0.86       396\n",
      "          14       0.83      0.92      0.87       394\n",
      "          15       0.82      0.87      0.84       398\n",
      "          16       0.73      0.90      0.81       364\n",
      "          17       0.96      0.83      0.89       376\n",
      "          18       0.78      0.61      0.68       310\n",
      "          19       0.66      0.66      0.66       251\n",
      "\n",
      "    accuracy                           0.80      7532\n",
      "   macro avg       0.81      0.80      0.80      7532\n",
      "weighted avg       0.81      0.80      0.80      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(('\\n\\nDeep Neural Network - Train accuracy:'),(round(accuracy_score(y_train, y_train_predclass),3)))\n",
    "print(('\\nDeep Neural Network - Test accuracy:'),(round(accuracy_score(y_test,y_test_predclass),3)))\n",
    "\n",
    "print('\\nDeep Neural Network - Train Classification Report')\n",
    "print(classification_report(y_train, y_train_predclass))\n",
    "\n",
    "print('\\nDeep Neural Network - Test classification Report')\n",
    "print(classification_report(y_test,y_test_predclass))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
